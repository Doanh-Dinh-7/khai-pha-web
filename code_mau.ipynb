{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from urllib.parse import urlsplit\n",
    "from nltk.corpus import stopwords\n",
    "from pyvi import ViTokenizer\n",
    "from textblob import TextBlob\n",
    "import shutil\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pyvi import ViTokenizer, ViPosTagger #pip install underthesea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://plo.vn/phap-luat/'\n",
    "file_store = \"D:/22/Cau1/Data\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "contents = soup.find_all('article', class_='story')\n",
    "data_crawl = []\n",
    "\n",
    "for content in contents:\n",
    "    if content.has_attr('data-id'):\n",
    "        data_crawl.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i=1\n",
    "while i <= 10:\n",
    "    Tieude = data_crawl[i].find('a', class_='cms-link')\n",
    "    print(Tieude.text)\n",
    "    Ngay = data_crawl[i].find('time', class_ = 'time')\n",
    "    Tomtat = data_crawl[i].find('div', class_ = 'story__summary story__shorten')\n",
    "\n",
    "    if Tieude or Ngay or Tomtat is not None:\n",
    "        link = Tieude.get('href')\n",
    "        title = Tieude['title']\n",
    "        time = Ngay['datetime']\n",
    "        desc = Tomtat.text\n",
    "\n",
    "        print(link)\n",
    "        print(title)\n",
    "        print(time)\n",
    "        print(desc)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    filename=os.path.join(file_store, f\"22_news{i}.txt\")   \n",
    "    with open(filename, 'w',encoding='utf-8') as f:\n",
    "        f.write(f\"{link}\" + \"\\n\")\n",
    "        f.write(f\"{title}\" + \"\\n\")\n",
    "        f.write(f\"{time}\" + \"\\n\")\n",
    "        f.write(f\"{desc}\")\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thư mục nguồn\n",
    "source_dir = 'D:/22/Cau2/Data'\n",
    "\n",
    "# Thư mục đích\n",
    "destination_dir = 'D:/22/Cau2/Data/Preprocessing'\n",
    "\n",
    "\n",
    "# Hàm xóa các thẻ HTML không cần thiết\n",
    "def XoaTag(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    for data in soup(['style', 'script']):\n",
    "        data.decompose()\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "\n",
    "i=1\n",
    "\n",
    "for file in os.listdir(source_dir):\n",
    "    try:\n",
    "        # Đọc nội dung của tệp\n",
    "        with open(os.path.join(source_dir, file), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        text_pre=text.replace(\"\\n\",\"\")    \n",
    "        text_pre=text.lower() # Chuyển văn bản thành chữ thường    \n",
    "        text_pre = re.sub(r\"http(s)?://\\S+\", \"\", text_pre) # Loại bỏ URL\n",
    "        text_pre = XoaTag(text_pre) # Xoá thẻ HTML\n",
    "        text_pre=re.sub(r'[^\\w\\s]','',text_pre) # Xóa dấu câu\n",
    "        text_pre = re.sub(\"\\d+\", \" \", text_pre) # Xóa chữ số\n",
    "        text_pre = re.sub(r\"[!@#$[]()]'\", \"\", text_pre) # Xoá kí tự đặc biệt: !@#$[]()\n",
    "        text_pre = nltk.sent_tokenize(text_pre) # Tách câu\n",
    "        text_pre = ''.join(text_pre)\n",
    "\n",
    "        # Xóa các từ không có nghĩa (Stop Words);\n",
    "        path=os.path.join(\"D:/22\")\n",
    "        f = open(path+r\"/vietnamese-stopwords.txt\", \"r\", encoding=\"utf-8\")\n",
    "        List_StopWords=f.read().split(\"\\n\")\n",
    "        text_pre=\" \".join(text_pre for text_pre in text_pre.split() if text_pre not in List_StopWords)\n",
    "\n",
    "        text_pre = ViTokenizer.tokenize(text_pre) # Tách từ (Tokenizing)\n",
    "\n",
    "        print (text_pre)\n",
    "\n",
    "        # Ghi nội dung đã được tiền xử lý vào một tệp mới\n",
    "        with open(os.path.join(destination_dir, f'22_Pre{i}.txt'), 'w',encoding='utf-8') as f:\n",
    "            for word in text_pre:\n",
    "                f.write(word)\n",
    "        i+=1\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tan Suat Tu, WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filestore = 'D:/22/Cau3/Data'\n",
    "\n",
    "text_pre = ''\n",
    "for file in os.listdir(filestore):\n",
    "  # Đọc nội dung của tệp\n",
    "    with open(os.path.join(filestore, file), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text_pre += text\n",
    "\n",
    "text_pre1=nltk.word_tokenize(text_pre) # Tokenizing\n",
    "\n",
    "print(\"Number of words: \",len(text_pre1))\n",
    "\n",
    "# Compute the frequency of all words / Tính tần số của tất cả các từ\n",
    "frequency_dist = FreqDist(word.lower() for word in text_pre1)\n",
    "\n",
    "## show only th top 50 results\n",
    "print(frequency_dist.most_common(50))\n",
    "\n",
    "## Consider words with length greater than 3 and plot\n",
    "# Xem xét các từ có độ dài lớn hơn 3 và vẽ\n",
    "large_words = dict([(k,v) for k,v in frequency_dist.items() if len(k)>3])\n",
    "frequency_dist = nltk.FreqDist(large_words)\n",
    "frequency_dist.plot(50,cumulative=False) # top 50 từ đầu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD CLOUD\n",
    "# Convert the text to a dictionary of word frequencies / Chuyển đổi văn bản thành từ điển tần số từ\n",
    "word_counts = nltk.FreqDist(text_pre.split())\n",
    "# Generate the word cloud / Tạo đám mây từ\n",
    "wcloud = WordCloud().generate_from_frequencies(frequency_dist)\n",
    "#plotting the wordcloud\n",
    "plt.imshow(wcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Tuong Dong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    filename = os.path.join(Path, filename)\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        data = f.read()  # Đọc dữ liệu như một chuỗi\n",
    "\n",
    "    return data  # Trả về chuỗi mà không mã hóa lại thành bytes\n",
    "\n",
    "\n",
    "# Chuẫn hóa dữ liệu\n",
    "def create_dataframe(matrix, tokens):\n",
    "    doc_names = [f'doc_{i+1}' for i, _ in enumerate(matrix)]\n",
    "    df = pd.DataFrame(data=matrix, index=doc_names, columns=tokens)\n",
    "    return(df)\n",
    "\n",
    "# # Tiền xử lý văn bản\n",
    "# import re\n",
    "# def Text_Preprocessing(doc):\n",
    "#     text_pre = doc.lower()\n",
    "#     text_pre=text_pre.replace(\"\\n\",\"\")    \n",
    "#     text_pre=text_pre.lower() # Chuyển văn bản thành chữ thường    \n",
    "#     text_pre = re.sub(r\"http(s)?://\\S+\", \"\", text_pre) # Loại bỏ URL\n",
    "#     text_pre=re.sub(r'[^\\w\\s]','',text_pre) # Xóa dấu câu\n",
    "#     text_pre = re.sub(\"\\d+\", \" \", text_pre) # Xóa chữ số\n",
    "#     text_pre = re.sub(r\"[!@#$[]()]'\", \"\", text_pre) # Xoá kí tự đặc biệt: !@#$[]()\n",
    "#     text_pre = ''.join(text_pre)\n",
    "\n",
    "#     # # Xóa các từ không có nghĩa (Stop Words);\n",
    "#     # text_pre = ViTokenizer.tokenize(text_pre) # Tách từ (Tokenizing)\n",
    "#         ##Remove stop words\n",
    "#     filename=os.path.join(r\"D:\\Phuong\\KhaiPhaDuLieuWeb\\Chương 5\\vietnamese-stopwords.txt\")\n",
    "#     f = open(filename, \"r\", encoding=\"utf-8\")\n",
    "    \n",
    "#     # #Get Stop words Dictionaries\n",
    "#     List_StopWords=f.read().split(\"/n\")\n",
    "#     text_pre=\" \".join(text for text in text_pre.split() if text not in List_StopWords)\n",
    "    \n",
    "#     text_pre = ViTokenizer.tokenize(text_pre)\n",
    "\n",
    "#     return text_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thêm tất cả các từ trong tập văn bản vào một tập hợp\n",
    "doc_ = []\n",
    "for i in range(1,4):\n",
    "    filestore = f'D:/Phuong/KhaiPhaDuLieuWeb/Chương 5/BTCh5/{i}'\n",
    "    for file in os.listdir(filestore):\n",
    "        text_pre =''\n",
    "        with open(os.path.join(filestore, file), 'r', encoding='utf-16 LE') as f:\n",
    "            text = f.read()\n",
    "            text_pre += text\n",
    "        doc_.append(text_pre)\n",
    "\n",
    "doc_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for doc in doc_:\n",
    "    doc_pre=Text_Preprocessing(doc)\n",
    "    data.append(doc_pre)\n",
    "\n",
    "data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng vector TF-IDF\n",
    "# data = [' '.join(doc) for doc in data]\n",
    "CountVect = CountVectorizer()\n",
    "vector_matrix = CountVect.fit_transform(data)\n",
    "vector_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy tên các từ (features)\n",
    "tokens = CountVect.get_feature_names_out()\n",
    "# Tạo DataFrame từ vector_matrix\n",
    "vector_df = create_dataframe(vector_matrix.toarray(), tokens)\n",
    "\n",
    "# Tính toán độ tương đồng cosine giữa các tài liệu\n",
    "cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
    "\n",
    "# Tạo doc_names từ doc_1 đến doc_n\n",
    "doc_names = [f'doc_{i+1}' for i in range(cosine_similarity_matrix.shape[0])]\n",
    "\n",
    "# Tạo DataFrame từ ma trận cosine similarity\n",
    "Similarity = create_dataframe(cosine_similarity_matrix, doc_names)\n",
    "\n",
    "# In ra DataFrame tương đồng cosine\n",
    "print(Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Tạo DataFrame từ ma trận cosine similarity\n",
    "Similarity_df = pd.DataFrame(Similarity, index=doc_names, columns=doc_names)\n",
    "\n",
    "# Chuyển DataFrame thành dạng long-form\n",
    "similarity_long = Similarity_df.stack().reset_index()\n",
    "similarity_long.columns = ['doc_1', 'doc_2', 'Do_tuong_dong']\n",
    "\n",
    "# Loại bỏ các dòng có doc_1 == doc_2 (để tránh độ tương đồng của tài liệu với chính nó)\n",
    "similarity_long = similarity_long[similarity_long['doc_1'] != similarity_long['doc_2']]\n",
    "\n",
    "# Sắp xếp theo độ tương đồng tăng dần\n",
    "similarity_sorted = similarity_long.sort_values(by='Do_tuong_dong', ascending=False)\n",
    "\n",
    "# In ra kết quả\n",
    "print(similarity_sorted)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

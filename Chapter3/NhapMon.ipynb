{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slide 15/37 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is introduction to nlp. it is likely to be useful, to people, machine learning is the new electrcity. there would be less hype around ai and more action going forward,python is the best tool! r is good langauage, i like this book, i want more books like this.\n"
     ]
    }
   ],
   "source": [
    "text=\"This is introduction to NLP. It is likely to be useful, to people, Machine learning is the new electrcity. There would be less hype around AI and more action going forward,python is the best tool! R is good langauage, I like this book, I want more books like this.\"\n",
    "\n",
    "text_pre=text.lower()\n",
    "print(text_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slide 16/37 3.2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is introduction to NLP It is likely to be useful to people Machine learning is the new electrcity There would be less hype around AI and more action going forwardpython is the best tool R is good langauage I like this book I want more books like this\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text=\"This is introduction to NLP. It is likely to be useful, to people, Machine learning is the new electrcity. There would be less hype around AI and more action going forward,python is the best tool! R is good langauage, I like this book, I want more books like this.\"\n",
    "\n",
    "#Cach 1: Su dung thu vien re\n",
    "text_pre=re.sub(r'[^\\w\\s]','',text)\n",
    "print(text_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slide 17/37 3.2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is introduction to NLP It is likely to be useful to people Machine learning is the new electrcity There would be less hype around AI and more action going forwardpython is the best tool R is good langauage I like this book I want more books like this\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text=\"This is introduction to NLP. It is likely to be useful, to people, Machine learning is the new electrcity. There would be less hype around AI and more action going forward,python is the best tool! R is good langauage, I like this book, I want more books like this.\"\n",
    "\n",
    "#Cach 2: Su dung thu vien string\n",
    "for c in string.punctuation:\n",
    "    text= text.replace(c,'')\n",
    "\n",
    "text_pre=text   \n",
    "print(text_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slide 18/37 3.2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·∫°i ƒê·∫°i h·ªôi Th·ªÉ thao ƒê√¥ng Nam √Å  , ƒê·ªôi tuy·ªÉn n·ªØ Vi·ªát Nam v√† ƒê·ªôi tuy·ªÉn U-  Vi·ªát Nam ƒë√£ b·∫£o v·ªá th√†nh c√¥ng t·∫•m huy ch∆∞∆°ng v√†ng t·∫°i SEA Games   sau chi·∫øn th·∫Øng l·∫ßn l∆∞·ª£t tr∆∞·ªõc ƒê·ªôi tuy·ªÉn n·ªØ Th√°i Lan ( - ) v√† ƒê·ªôi tuy·ªÉn U-  Th√°i Lan ( - ).\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text=\"T·∫°i ƒê·∫°i h·ªôi Th·ªÉ thao ƒê√¥ng Nam √Å 2021, ƒê·ªôi tuy·ªÉn n·ªØ Vi·ªát Nam v√† ƒê·ªôi tuy·ªÉn U-23 Vi·ªát Nam ƒë√£ b·∫£o v·ªá th√†nh c√¥ng t·∫•m huy ch∆∞∆°ng v√†ng t·∫°i SEA Games 30 sau chi·∫øn th·∫Øng l·∫ßn l∆∞·ª£t tr∆∞·ªõc ƒê·ªôi tuy·ªÉn n·ªØ Th√°i Lan (1-0) v√† ƒê·ªôi tuy·ªÉn U-23 Th√°i Lan (1-0).\"\n",
    "\n",
    "text_pre = re.sub(r\"\\d+\", \" \", text)\n",
    "print(text_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slide 19/37 3.2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khoa h·ªçc d·ªØ li·ªáu L√† m·ªôt ng√†nh h·ªçc ƒë√°p ·ª©ng xu h∆∞·ªõng vi·ªác l√†m trong t∆∞∆°ng lai.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "Text = \"<html><head><title>Khoa h·ªçc d·ªØ li·ªáu</title><style>.call {background-color:black;} </style><script>getit</script></head><body>L√† m·ªôt ng√†nh h·ªçc<div>ƒë√°p ·ª©ng xu h∆∞·ªõng vi·ªác l√†m trong t∆∞∆°ng lai.</div></body></html>\"\n",
    " \n",
    "# Function to remove tags\n",
    "def remove_tags(html): \n",
    "    soup = BeautifulSoup(html, \"html.parser\") # parse html content\n",
    "    for data in soup(['style', 'script']):\n",
    "        data.decompose() # Remove tags\n",
    "    return ' '.join(soup.stripped_strings)  # return data by retrieving the tag content\n",
    "  \n",
    "# Print the extracted data\n",
    "print(remove_tags(Text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slide 21/37 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are you saying face_with_tears_of_joy. I am the boss Happy_face_or_smiley, and why are you so unamused_face\n"
     ]
    }
   ],
   "source": [
    "#Installing emot library\n",
    "#pip install emot\n",
    "from emot.emo_unicode import UNICODE_EMOJI # For emojis\n",
    "from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS\n",
    " \n",
    "# Function for converting emojis into word\n",
    "def converting_emojis(text):\n",
    "    for x in EMOTICONS_EMO:\n",
    "        text = text.replace(x, \"_\".join(EMOTICONS_EMO[x].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "        \n",
    "    for x in UNICODE_EMOJI:\n",
    "        text = text.replace(x, \"_\".join(UNICODE_EMOJI[x].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "            \n",
    "    return text\n",
    "\n",
    "text = \"What are you saying üòÇ. I am the boss :), and why are you so üòí\"\n",
    "text_pre=converting_emojis(text)\n",
    "print(text_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slide 23/37 3.4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This introduction NLP. It likely useful, people, Machine learning new electrcity. There would less hype around AI action going forward,python best tool! R good langauage, I like book, I want books like this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#install and import libraries\n",
    "#Download nltk libraries\n",
    "#nltk.download()\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text=\"This is introduction to NLP. It is likely to be useful, to people, Machine learning is the new electrcity. There would be less hype around AI and more action going forward,python is the best tool! R is good langauage, I like this book, I want more books like this.\"\n",
    "\n",
    "#remove stop words\n",
    "stop = stopwords.words('english')\n",
    "text_pre = \" \".join(text for text in text.split() if text not in stop)\n",
    "\n",
    "print(text_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slide 24/37 2.4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theo gi·ªõi chuy√™n m√¥n, l·ªëi U20 Vi·ªát Nam ƒëa d·∫°ng, linh ho·∫°t s∆° ƒë·ªì ƒë·ªôi h√¨nh chi·∫øn thu·∫≠t, kh·∫£ nƒÉng t√¨nh c√¥ng ph√≤ng ng·ª±.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "text=\"Theo gi·ªõi chuy√™n m√¥n, l·ªëi ch∆°i c·ªßa U20 Vi·ªát Nam kh√° ƒëa d·∫°ng, linh ho·∫°t h∆°n trong s∆° ƒë·ªì ƒë·ªôi h√¨nh v√† chi·∫øn thu·∫≠t, c√≥ kh·∫£ nƒÉng chuy·ªÉn t√¨nh th·∫ø nhanh trong t·∫•n c√¥ng v√† ph√≤ng ng·ª±.\"\n",
    "\n",
    "# path=os.path.dirname(__file__)\n",
    "# ƒê∆∞·ªùng d·∫´n t·ªõi file stop words\n",
    "file_path = os.path.join(\"Data\", \"vietnamese-stopwords.txt\")\n",
    "\n",
    "# M·ªü file ch·ª©a c√°c stop words\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    # ƒê·ªçc c√°c stop words v√†o danh s√°ch\n",
    "    List_StopWords = f.read().split(\"\\n\")\n",
    "\n",
    "#remove stop words\n",
    "text_pre=\" \".join(text for text in text.split() if text not in List_StopWords)\n",
    "print(text_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slide 25/37 3.5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U20 Qatar v·∫´n c√≤n c∆° h·ªôi ƒëi ti·∫øp d√π kh√° mong manh', ' H·ªç ph·∫£i th·∫Øng √öc ƒë·∫≠m v√† ƒë·ª£i ch·ªù U20 Vi·ªát Nam ƒë√°nh b·∫°i Iran', ' Khi ƒë√≥, Qatar m·ªõi c√≥ th·ªÉ nghƒ© ƒë·∫øn chuy·ªán ƒëo·∫°t v√© v√†o v√≤ng trong', ' ƒê√≥ l√† c∆° s·ªü ƒë·ªÉ U20 Vi·ªát Nam k·ª≥ v·ªçng v√†o t·∫•m v√© ƒëi ti·∫øp ngay c·∫£ khi kh√¥ng th·ªÉ c√≥ k·∫øt qu·∫£ t·ªët tr∆∞·ªõc Iran']\n"
     ]
    }
   ],
   "source": [
    "text=\"U20 Qatar v·∫´n c√≤n c∆° h·ªôi ƒëi ti·∫øp d√π kh√° mong manh. H·ªç ph·∫£i th·∫Øng √öc ƒë·∫≠m v√† ƒë·ª£i ch·ªù U20 Vi·ªát Nam ƒë√°nh b·∫°i Iran. Khi ƒë√≥, Qatar m·ªõi c√≥ th·ªÉ nghƒ© ƒë·∫øn chuy·ªán ƒëo·∫°t v√© v√†o v√≤ng trong. ƒê√≥ l√† c∆° s·ªü ƒë·ªÉ U20 Vi·ªát Nam k·ª≥ v·ªçng v√†o t·∫•m v√© ƒëi ti·∫øp ngay c·∫£ khi kh√¥ng th·ªÉ c√≥ k·∫øt qu·∫£ t·ªët tr∆∞·ªõc Iran.\"\n",
    "\n",
    "text_pre=text.split(\".\")\n",
    "text_pre.remove(\"\")\n",
    "print(text_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slide 36/37 3.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U20 Qatar v·∫´n c√≤n c∆° h·ªôi ƒëi ti·∫øp d√π kh√° mong manh.', 'H·ªç ph·∫£i th·∫Øng √öc ƒë·∫≠m v√† ƒë·ª£i ch·ªù U20 Vi·ªát Nam ƒë√°nh b·∫°i Iran.', 'Khi ƒë√≥, Qatar m·ªõi c√≥ th·ªÉ nghƒ© ƒë·∫øn chuy·ªán ƒëo·∫°t v√© v√†o v√≤ng trong.', 'ƒê√≥ l√† c∆° s·ªü ƒë·ªÉ U20 Vi·ªát Nam k·ª≥ v·ªçng v√†o t·∫•m v√© ƒëi ti·∫øp ngay c·∫£ khi kh√¥ng th·ªÉ c√≥ k·∫øt qu·∫£ t·ªët tr∆∞·ªõc Iran.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text=\"U20 Qatar v·∫´n c√≤n c∆° h·ªôi ƒëi ti·∫øp d√π kh√° mong manh. H·ªç ph·∫£i th·∫Øng √öc ƒë·∫≠m v√† ƒë·ª£i ch·ªù U20 Vi·ªát Nam ƒë√°nh b·∫°i Iran. Khi ƒë√≥, Qatar m·ªõi c√≥ th·ªÉ nghƒ© ƒë·∫øn chuy·ªán ƒëo·∫°t v√© v√†o v√≤ng trong. ƒê√≥ l√† c∆° s·ªü ƒë·ªÉ U20 Vi·ªát Nam k·ª≥ v·ªçng v√†o t·∫•m v√© ƒëi ti·∫øp ngay c·∫£ khi kh√¥ng th·ªÉ c√≥ k·∫øt qu·∫£ t·ªët tr∆∞·ªõc Iran.\"\n",
    "\n",
    "sentence_list = nltk.sent_tokenize(text)\n",
    "print(sentence_list)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slide 28/37 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I like natural language processing it's your choice.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "lookup_dict = {'nlp':'natural language processing', 'ur':'your', \"wbu\" : \"what about you\"}\n",
    "\n",
    "def text_std(input_text):\n",
    "    words = input_text.split()\n",
    "    # Get Abbreviations Words\n",
    "    text_pre=\"\"\n",
    "    for word in words:\n",
    "        w=word\n",
    "        w = re.sub(r'[^\\w\\s]','',w) #Removing Punctuation\n",
    "        if w.lower() in lookup_dict:\n",
    "            word=lookup_dict[w]\n",
    "        text_pre=text_pre + \" \" + word        \n",
    "    return text_pre\n",
    "\n",
    "text_pre=text_std(\"I like nlp it's ur choice.\")\n",
    "print(text_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slide 29/37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFG is a good company and always value their employees.\n"
     ]
    }
   ],
   "source": [
    "#Install textblob library\n",
    "#pip install textblob\n",
    "\n",
    "#import libraries and use 'correct' function\n",
    "from textblob import TextBlob\n",
    "text=\"GFG is a good compny and alays value ttheir employees.\"\n",
    "\n",
    "text_pre=TextBlob(text).correct()\n",
    "print(text_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install autocorrect library\n",
    "# pip install autocorrect\n",
    "from autocorrect import spell\n",
    "text=\"GFG is a good compny and alays value ttheir employees.\"\n",
    "\n",
    "text_pre=spell(text)\n",
    "print(text_pre)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
